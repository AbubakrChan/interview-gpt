{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed08ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3c20da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/81/d7/d2537d51efb692d0c411e64267ba349e7668d40f5bc73cefe78ccd650dcd/scipy-1.11.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached scipy-1.11.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from scipy) (1.26.0)\n",
      "Using cached scipy-1.11.3-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.11.3\n",
      "Requirement already satisfied: tenacity in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (8.2.3)\n",
      "Collecting tiktoken\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/b8/eb/234646d9eefda8a500d0fd88b05bf625a90ed18054124349db26e558276e/tiktoken-0.5.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tiktoken-0.5.1-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Obtaining dependency information for regex>=2022.1.18 from https://files.pythonhosted.org/packages/b8/ad/3398312096118c4e62a5827664e52a04d5068e84d04142dd4a0da8a567ae/regex-2023.10.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached regex-2023.10.3-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Using cached tiktoken-0.5.1-cp311-cp311-win_amd64.whl (759 kB)\n",
      "Using cached regex-2023.10.3-cp311-cp311-win_amd64.whl (269 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.10.3 tiktoken-0.5.1\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: termcolor\n",
      "Successfully installed termcolor-2.3.0\n",
      "Requirement already satisfied: openai in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from openai) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests>=2.20->openai) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests>=2.20->openai) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: requests in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\work\\code\\interview-prep-bot\\.env\\lib\\site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install tenacity\n",
    "!pip install tiktoken\n",
    "!pip install termcolor \n",
    "!pip install openai\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dd6486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "#from langchain import SerpAPIWrapper, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import List, Union\n",
    "#from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "#from langchain.memory import ConversationBufferWindowMemory\n",
    "import openai\n",
    "import re\n",
    "import streamlit as st\n",
    "\n",
    "from config import CHAT_MODEL, SYSTEM_PROMPT, CHATGPT_ANSWER_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b9f0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from termcolor import colored\n",
    "\n",
    "#GPT_MODEL = \"gpt-3.5-turbo-0613\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70ba500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env.local\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2c5a3",
   "metadata": {},
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, function_call=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + OPENAI_API_KEY,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"messages\": messages}\n",
    "    if functions is not None:\n",
    "        json_data.update({\"functions\": functions})\n",
    "    if function_call is not None:\n",
    "        json_data.update({\"function_call\": function_call})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe037f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    \n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45053d5a",
   "metadata": {},
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous or value is missing\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Onboard user nikhil.utane\"})\n",
    "chat_response = chat_completion_request(\n",
    "    messages, \n",
    ")\n",
    "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d57c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_answer(question, rough_answer):\n",
    "\n",
    "    chatgpt_answer_prepped = CHATGPT_ANSWER_PROMPT.format(\n",
    "        QUESTION_HERE=question, ROUGH_ANSWER_HERE=rough_answer\n",
    "    )    \n",
    "\n",
    "    final_answer = openai.ChatCompletion.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                  {\"role\": \"user\", \"content\": chatgpt_answer_prepped}],\n",
    "        max_tokens=1000,\n",
    "    )    \n",
    "    # Response provided by GPT-3.5\n",
    "    return final_answer[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ba8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_answers(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if \"sample\" in dirs:\n",
    "            dirs.remove(\"sample\")\n",
    "        for dir in dirs:\n",
    "            #print(os.path.join(root, dir))\n",
    "            #print(dir)\n",
    "            file_path =  os.path.join(root, dir) + '/'\n",
    "            with open( file_path + '0-question.txt', 'r') as file:\n",
    "                question = file.read()\n",
    "                #print(question)\n",
    "                \n",
    "            with open( file_path + '1-rough-answer.txt', 'r') as file:\n",
    "                rough_answer = file.read()\n",
    "                #print(question)\n",
    "                \n",
    "            final_answer = generate_final_answer(question, rough_answer)    \n",
    "            #print(final_answer)\n",
    "            #print(rough_answer)\n",
    "            with open( file_path + '2-chatgpt-answer.txt', 'w') as file:\n",
    "                file.write(final_answer)\n",
    "                print(file_path + '2-chatgpt-answer.txt' + \" updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a4ede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qna\\career_overview/2-chatgpt-answer.txt updated\n",
      "qna\\weakness/2-chatgpt-answer.txt updated\n"
     ]
    }
   ],
   "source": [
    "generate_all_answers(\"qna\")\n",
    "#print(openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c66fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408daf80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047df4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21e19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bab17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb0e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
